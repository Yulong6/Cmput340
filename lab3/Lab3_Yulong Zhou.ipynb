{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Least Squares Lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYpBuYuVnDG"
      },
      "source": [
        "<h2>Lab 3: Least Squares</h2>\n",
        "\n",
        "<b>Demo Date: </b> Oct. 13 <br>\n",
        "<b>Due Date: </b> Oct. 15\n",
        "\n",
        "In this lab you will implement two algorithms for solving an ill-conditioned least squares problem. We create an artificial and overdetermined least-squares problem by removing two columns of a $10 \\times 10$ Hilbert matrix, a classic ill-conditioned matrix. As we remove the two columns from the matrix it is no longer a Hilbert matrix, but it creates an overdetermined system with a large condition number: $\\approx 3,796,554,168$.\n",
        "\n",
        "Implement an algorithm that you believe will compute the value of $x$ for the least squares problem $Ax \\approx b$ as accurately as the problem allows. You will also implement an algorithm that you believe to produce inaccurate solutions due to the large condition number of the system. In this lab you do not need to worry about having a fast implementation, any non-vectorized implementation will be enough. \n",
        "\n",
        "Compare the two solutions in terms of the norm of their residual vector $Ax - b$. In the demo of your lab you should be able to explain your choices of algorithms and the results you obtained. Why did one method produce a solution with a smaller residual than the other? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPj5ANjHVnDH",
        "outputId": "4b0ae098-2000-4c4b-cfb8-014624c85eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "from scipy.linalg import hilbert\n",
        "from scipy.optimize import lsq_linear\n",
        "\n",
        "b = np.ones(10)\n",
        "A = hilbert(10)\n",
        "A = np.delete(A, 5, 1)\n",
        "A = np.delete(A, 4, 1)\n",
        "\n",
        "print('Condition number of A: ', np.linalg.norm(A) * np.linalg.norm(np.linalg.pinv(A)))\n",
        "\n",
        "\n",
        "b_1 = np.ones(10)\n",
        "A_1 = hilbert(10)\n",
        "A_1 = np.delete(A, 5, 1)\n",
        "A_1 = np.delete(A, 4, 1)\n",
        "\n",
        "print('Condition number of A1: ', np.linalg.norm(A_1) * np.linalg.norm(np.linalg.pinv(A_1)))\n",
        "\n",
        "# Use this test instance while developing the algorithms. \n",
        "# This instance is much easier to debug than the ill-conditioned problem above. \n",
        "A_test = np.array([[1, 2, 2], [4, 4, 2], [4, 6, 4]], dtype=float)\n",
        "A_test2 = np.array([[1, 2, 2], [4, 4, 2], [4, 6, 4]], dtype=float)\n",
        "b_test = np.array([3, 6, 10], dtype=float)\n",
        "b_test2 = np.array([3, 6, 10], dtype=float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Condition number of A:  3796554168.541331\n",
            "Condition number of A1:  51447888.49081164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fGXKdAXjhdC"
      },
      "source": [
        " def back_substituion(U, b):\n",
        "  n= len(U)\n",
        "  x=np.zeros(n)\n",
        "  for j in range(n-1,-1,-1):\n",
        "    if U[j][j]==0:\n",
        "      break\n",
        "    x[j] = b[j]/U[j][j]\n",
        "    for i in range(j):\n",
        "      b[i] = b[i] - U[i][j]*x[j]\n",
        "  return x\n",
        "\n",
        "def forward_substituion(L, b):\n",
        "  n=len(L)\n",
        "  y=np.zeros(n)\n",
        "  for j in range(n):\n",
        "    if L[j][j]==0:\n",
        "      break\n",
        "    y[j] = b[j]/L[j][j]\n",
        "    for i in range(j+1,n):\n",
        "      b[i] = b[i] - L[i][j]*y[j]\n",
        "  return y    \n",
        "\n",
        "def calculate(Q,b):\n",
        "  Q_T = np.transpose(Q)\n",
        "  C = np.dot(Q_T,b)\n",
        "  #print(\"C is:\",C)\n",
        "  return C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqSEloFQjnxJ",
        "outputId": "92566fcb-2065-4ec1-ba67-6be1aeb238df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def accurate(A): #MGS\n",
        "  n = A.shape[1]\n",
        "  Q = np.zeros((A.shape[0],A.shape[0])) #10*10\n",
        "  R = np.zeros((n,n))  #8*8\n",
        "\n",
        "  for i in range(0,n):  #loop over column\n",
        "    R[i][i] = np.linalg.norm(A[:,i])\n",
        "    if R[i][i] == 0:  #stop if linearly dependent\n",
        "      break\n",
        "    else:\n",
        "      Q[:,i] = A[:,i]/R[i][i]  #normalize current column\n",
        "      for j in range(i+1,n):  #subtract from succeeding columns them components in current column\n",
        "        q_T = np.transpose(Q[:,i])\n",
        "        R[i][j] = np.dot(q_T,A[:,j])\n",
        "        A[:,j] -= R[i][j]*Q[:,i]\n",
        "\n",
        "#  print(\"Q is:\",Q)\n",
        "#  print(\"R is:\",R)\n",
        "  return Q,R\n",
        "\n",
        "A1 = A\n",
        "b1 = b\n",
        "Q_a,R_a = accurate(A1)\n",
        "C_a = calculate(Q_a,b1)\n",
        "x_a = back_substituion(R_a.T,C_a)\n",
        "residual = np.linalg.norm(np.dot(A1,x_a) - b1)\n",
        "print(\"residual is:\",residual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "residual is: 1.3094887165295381e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdRDFq7xozio",
        "outputId": "1fe76d15-d696-43d4-898d-599985731e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def inaccurate(A): #CGS\n",
        "  n = A.shape[1]\n",
        "  Q = np.zeros((A.shape[0],A.shape[0])) #10*10\n",
        "  R = np.zeros((n,n))  #8*8\n",
        "\n",
        "  for i in range(0,n):  #loop over columns\n",
        "    Q[:,i] = A[:,i]\n",
        "    for j in range(0,i):  #subtract from current column its components in preceding columns\n",
        "      Q_T = np.transpose(Q[:,j])\n",
        "      R[j][i] = np.dot(Q_T,A[:,i])\n",
        "      Q[:,i] -= R[j][i]*Q[:,j]\n",
        "    R[i][i] = np.linalg.norm(Q[:,i])\n",
        "    if R[i][i] == 0:  #stop if linear dependent\n",
        "      break\n",
        "    Q[:,i] = Q[:,i]/R[i][i]  #normalize current column\n",
        "  \n",
        "  return Q,R\n",
        "\n",
        "A2 = A_1\n",
        "b2 = b_1\n",
        "Q_i,R_i =inaccurate(A2)\n",
        "C_i = calculate(Q_i,b2)\n",
        "x_i = back_substituion(R_i,C_i)\n",
        "residual = np.linalg.norm(np.dot(A2,x_i) - b2)\n",
        "print(\"residual is:\",residual)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "residual is: 0.0002685582571750269\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}